# ğŸš€ Proxy Server starten

## Status

âœ… **tf-keras installiert** - TensorFlow-Problem behoben  
âœ… **Test erfolgreich** - Alle 3 Layer funktionieren  
âš ï¸ **FastAPI prÃ¼fen** - MÃ¶glicherweise nicht installiert

---

## Schnellstart

### Schritt 1: FastAPI installieren (falls nicht vorhanden)

```powershell
pip install fastapi uvicorn httpx
```

### Schritt 2: Proxy starten

```powershell
cd standalone_packages/llm-security-firewall
python src/proxy_server.py
```

**Erwartete Ausgabe:**
```
======================================================================
ğŸ›¡ï¸  Guardian Firewall Proxy Server
======================================================================
ğŸ“ Port: 8080
ğŸ“š Allowed topics: ['Mathe', 'Physik', 'Chemie', 'Biologie']
ğŸ”’ RC10b detector: âœ… enabled
ğŸ‘¶ Kids Policy Truth Validator: âš ï¸  disabled
ğŸ¤– Ollama integration: âš ï¸  disabled (mock mode)
======================================================================
ğŸš€ Server ready! Waiting for requests...
======================================================================
```

**Hinweis**: Wenn Ollama nicht installiert ist, lÃ¤uft der Server im **Mock-Modus** (kein Problem fÃ¼r Tests).

### Schritt 3: Testen (in neuem Terminal)

```powershell
# Test 1: Sichere Anfrage
python scripts/test_proxy_live.py

# Oder manuell mit PowerShell:
$body = @{message="Was ist 2+2?"; age_band="9-12"} | ConvertTo-Json
Invoke-RestMethod -Uri "http://localhost:8080/proxy/chat" -Method Post -Body $body -ContentType "application/json"
```

---

## Was du siehst

### Bei sicheren Anfragen:
- âœ… Layer 1 (TopicFence): ON_TOPIC
- âœ… Layer 2A (RC10b): ALLOWED
- âœ… Layer 2B (Kids Input): SAFE
- âœ… Response: Mock-Response (oder Ollama, falls installiert)

### Bei unsicheren Anfragen:
- âœ… Layer 1: ON_TOPIC (kann passieren)
- âœ… Layer 2A: ALLOWED (Single-Request)
- âŒ Layer 2B: **BLOCKED** (Keyword erkannt)
- âœ… Response: Safety Template (keine LLM-Antwort)

---

## NÃ¤chste Schritte

1. **Jetzt**: Proxy starten und testen (Mock-Modus ist OK)
2. **Optional**: Ollama installieren fÃ¼r echte LLM-Responses
3. **SpÃ¤ter**: Streaming, Web-UI, etc.

**Der Code ist bereit. Starte einfach den Server!** ğŸ¯

