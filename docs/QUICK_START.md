# üöÄ Quick Start - Guardian Firewall Proxy

## Schritt 1: Proxy-Server starten

**Terminal 1** (Server):

```powershell
cd "D:\MCP Mods\HAK_GAL_HEXAGONAL\standalone_packages\llm-security-firewall"
python src/proxy_server.py
```

**Erwartete Ausgabe:**
```
======================================================================
üõ°Ô∏è  Guardian Firewall Proxy Server
======================================================================
üìç Port: 8080
ü§ñ Ollama integration: ‚ö†Ô∏è  disabled (mock mode)
======================================================================
üöÄ Server ready! Waiting for requests...
======================================================================
```

**Wichtig**: Lass dieses Terminal offen! Der Server l√§uft hier.

---

## Schritt 2: Testen (in neuem Terminal)

**Terminal 2** (Test):

```powershell
cd "D:\MCP Mods\HAK_GAL_HEXAGONAL\standalone_packages\llm-security-firewall"
python scripts/test_proxy_live.py
```

**Oder manuell mit PowerShell:**

```powershell
$body = @{
    message = "Was ist 2+2?"
    age_band = "9-12"
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:8080/proxy/chat" `
    -Method Post `
    -Body $body `
    -ContentType "application/json" `
    -Headers @{"X-Session-ID"="test-1"}
```

---

## Was du siehst

### Bei sicheren Anfragen:
- ‚úÖ Status: `ALLOWED`
- ‚úÖ Response: Mock-Response (oder Ollama, falls installiert)
- ‚úÖ LLM Provider: `mock` oder `ollama`

### Bei unsicheren Anfragen:
- ‚ùå Status: `BLOCKED_UNSAFE` oder `BLOCKED_OFF_TOPIC`
- ‚úÖ Response: Safety Template
- ‚úÖ **Ollama wird NICHT aufgerufen!**

---

## Troubleshooting

### "Konnte nicht zum Proxy verbinden"

- Pr√ºfe, ob der Server in Terminal 1 l√§uft
- Pr√ºfe Port 8080: `netstat -ano | findstr :8080`
- Starte den Server neu: `python src/proxy_server.py`

### "ModuleNotFoundError: No module named 'fastapi'"

```powershell
pip install fastapi uvicorn httpx
```

### Ollama-Integration (Optional)

Falls du echte LLM-Responses willst:

1. Installiere Ollama: [Ollama Download](https://ollama.ai/download)
2. Lade Modell: `ollama pull llama3`
3. Starte Proxy neu ‚Üí Erkennt Ollama automatisch

---

## N√§chste Schritte

1. ‚úÖ **Jetzt**: Server starten und testen
2. **Optional**: Ollama installieren f√ºr echte LLM-Responses
3. **Sp√§ter**: Streaming, Web-UI, Docker, etc.

**Der Code ist bereit. Starte einfach den Server!** üéØ
