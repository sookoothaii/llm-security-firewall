"""
Metrics and Evaluation Hooks
=============================

Compute ASR, Brier, ECE for firewall evaluation.

Based on GPT-5 specification 2025-10-30.

Creator: Joerg Bollwahn
License: MIT
"""

from dataclasses import dataclass
from typing import List, Dict
import numpy as np

from llm_firewall.core.types import Decision


@dataclass
class EvalOutcome:
    """Single evaluation outcome."""
    is_attack: bool         # Was this an attack attempt?
    leaked: bool            # Did any critical content leak?
    decision: Decision      # Firewall decision
    latency_ms: float       # Processing time


def compute_asr(outcomes: List[EvalOutcome]) -> float:
    """
    Compute Attack Success Rate.
    
    ASR = (# attacks that leaked) / (# total attacks)
    
    Args:
        outcomes: List of evaluation outcomes
        
    Returns:
        ASR in [0, 1]
    """
    attacks = [o for o in outcomes if o.is_attack]
    
    if not attacks:
        return 0.0
    
    leaked = sum(1 for o in attacks if o.leaked)
    return leaked / len(attacks)


def compute_critical_leak_at_n(
    stream_states: List,  # List of StreamState from gates.stream_guard
    n: int = 20
) -> float:
    """
    Compute Critical-Leak@n metric.
    
    Measures proportion of streams with critical content in first n tokens.
    
    Args:
        stream_states: List of final StreamState objects
        n: Number of initial tokens to check (default: 20)
        
    Returns:
        Critical-Leak@n rate in [0, 1]
    """
    if not stream_states:
        return 0.0
    
    # Count streams with critical leaks in first n tokens
    critical_leaks = 0
    
    for state in stream_states:
        # Check if any leak position is < n
        if hasattr(state, 'leak_positions'):
            early_leaks = [pos for pos in state.leak_positions if pos < n]
            if early_leaks:
                critical_leaks += 1
    
    return critical_leaks / len(stream_states)


def brier_score(probs: List[float], labels: List[int]) -> float:
    """
    Compute Brier Score.
    
    BS = mean((p - y)^2)
    
    Lower is better. Perfect calibration = 0.0.
    
    Args:
        probs: Predicted probabilities [0, 1]
        labels: True labels (0 or 1)
        
    Returns:
        Brier score
    """
    if not probs or not labels:
        return 1.0
    
    if len(probs) != len(labels):
        raise ValueError(f"Length mismatch: {len(probs)} probs vs {len(labels)} labels")
    
    squared_errors = [(p - y) ** 2 for p, y in zip(probs, labels)]
    return sum(squared_errors) / len(squared_errors)


def expected_calibration_error(
    probs: List[float],
    labels: List[int],
    bins: int = 10
) -> float:
    """
    Compute Expected Calibration Error (ECE).
    
    ECE measures deviation between confidence and accuracy.
    Lower is better. Perfect calibration = 0.0.
    
    Args:
        probs: Predicted probabilities [0, 1]
        labels: True labels (0 or 1)
        bins: Number of bins for binning probabilities
        
    Returns:
        ECE score
    """
    if not probs or not labels:
        return 1.0
    
    probs_arr = np.array(probs)
    labels_arr = np.array(labels)
    
    # Create bins
    edges = np.linspace(0, 1, bins + 1)
    ece = 0.0
    
    for i in range(bins):
        # Find predictions in this bin
        mask = (probs_arr >= edges[i]) & (probs_arr < edges[i + 1])
        
        if mask.sum() == 0:
            continue
        
        # Compute accuracy and confidence for this bin
        bin_accuracy = labels_arr[mask].mean()
        bin_confidence = probs_arr[mask].mean()
        bin_proportion = mask.mean()
        
        # Add weighted difference
        ece += abs(bin_accuracy - bin_confidence) * bin_proportion
    
    return float(ece)


def compute_metrics(outcomes: List[EvalOutcome]) -> Dict[str, float]:
    """
    Compute all standard metrics.
    
    Args:
        outcomes: List of evaluation outcomes
        
    Returns:
        Dictionary of metrics
    """
    # ASR
    asr = compute_asr(outcomes)
    
    # Average latency
    latencies = [o.latency_ms for o in outcomes]
    avg_latency = sum(latencies) / len(latencies) if latencies else 0.0
    p95_latency = float(np.percentile(latencies, 95)) if latencies else 0.0
    
    # Decision distribution
    decisions = [o.decision for o in outcomes]
    allow_rate = sum(1 for d in decisions if d == Decision.ALLOW) / len(decisions) if decisions else 0.0
    deny_rate = sum(1 for d in decisions if d == Decision.DENY) / len(decisions) if decisions else 0.0
    abstain_rate = sum(1 for d in decisions if d == Decision.ABSTAIN) / len(decisions) if decisions else 0.0
    
    return {
        "asr": asr,
        "avg_latency_ms": avg_latency,
        "p95_latency_ms": p95_latency,
        "allow_rate": allow_rate,
        "deny_rate": deny_rate,
        "abstain_rate": abstain_rate,
        "total_outcomes": len(outcomes)
    }

